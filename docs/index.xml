<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Lorenzo Fontana&#39;s Homepage</title>
    <link>https://fntlnz.wtf/</link>
    <language>en-US</language>
    <author>Lorenzo Fontana</author>
    <updated>Tue, 16 Jan 2018 13:03:00 &#43;0200</updated>
    
    <item>
      <title>Post-mortem debugging of Go Programs</title>
      <link>https://fntlnz.wtf/post/gopostmortem/</link>
      <pubDate>Tue, 16 Jan 2018 13:03:00 &#43;0200</pubDate>
      <author>Lorenzo Fontana</author>
      <guid>https://fntlnz.wtf/post/gopostmortem/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Even &lt;code&gt;if&lt;/code&gt; you write tests and those tests are perfect, effective and everything
you will face production issues. Sometimes everything will just crash and
when that&amp;rsquo;ll happen you&amp;rsquo;ll need some debugging skills.
Here I&amp;rsquo;m putting some flow examples, notes and resources around post-mortem debugging of
Go programs, including, but not limited to, those running inside a Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m a huge fan of GDB, I used it for almost a decade now and while I remember
it was a bit clunky and difficult at first but it got better in the years
and in the meantime I got used to it and it&amp;rsquo;s commands and perks.&lt;/p&gt;

&lt;p&gt;A few years after my first gdb, I saw Delve and had the same feeling: it was clunky,
and it had nearly anything I needed. Now, after a couple of years Delve seems pretty
complete and it just works, one thing I really like about Delve is that even
if the terminal interface is similar to other debuggers (like gdb) has a very
well done UX, for example:&lt;/p&gt;

&lt;p&gt;This is the current execution point of a program under Delve:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-delve&#34;&gt;    29:         for {
    30:                 select {
    31:                 case message := &amp;lt;-ch:
    32:                         fmt.Println(message)
    33:                 case &amp;lt;-time.After(time.Second * 3):
=&amp;gt;  34:                         panic(&amp;quot;I&#39;m panicking because of timeout!&amp;quot;)
    35:                 }
    36:         }
    37: }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is how the same thing looks like in GDB&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-gdb&#34;&gt;30                      select {
31                      case message := &amp;lt;-ch:
32                              fmt.Println(message)
33                      case &amp;lt;-time.After(time.Second * 3):
34                              panic(&amp;quot;I&#39;m panicking because of timeout!&amp;quot;)
35                      }
36              }
37      }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, one could extend GDB by customizing &lt;code&gt;.gdbinit&lt;/code&gt; like how the &lt;a href=&#34;https://github.com/cyrus-and/gdb-dashboard&#34;&gt;GDB Dashboard&lt;/a&gt; does:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://fntlnz.wtf/gdb-go/gdb-dashboard.jpg&#34; alt=&#34;GDB Dashboard Screenshot&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A part from the UX, Delve has several advantages over GDB when debugging Go:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It works out of the box - (Just Worksâ„¢)&lt;/li&gt;
&lt;li&gt;Integrated with major editors: Vim/Nvim, Code, Gogland&lt;/li&gt;
&lt;li&gt;It has an impressivly well working support for Go concurrency patterns&lt;/li&gt;
&lt;li&gt;Even post-mortem core dump analysis works just right&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On the other hand, GDB has still some advantages over it:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It supports any architecture (Delve only amd64)&lt;/li&gt;
&lt;li&gt;It supports any OS (Delve only Linux, OSX and Windows)&lt;/li&gt;
&lt;li&gt;You can debug Cgo&lt;/li&gt;
&lt;li&gt;You need to extend your debugging to the Go runtime itself, GDB is your tool&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, how to say, I&amp;rsquo;m not very religious about the debugger I use but generally
when I need to choose Delve or GDB I just go with Delve and then if Delve breaks
I try with GDB. It&amp;rsquo;s not very scientific but it&amp;rsquo;s a wise way to proceed.&lt;/p&gt;

&lt;p&gt;I will show a few examples:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You need to debug an API server entirely written in Go, delve&lt;/li&gt;
&lt;li&gt;You need to debug some Go program orchestrating linux namespaces (like runc), delve breaks and I use GDB because in to interact with kernel features you need Cgo!&lt;/li&gt;
&lt;li&gt;You need to debug a program compiled for ppc64le? delve breaks, I use GDB&lt;/li&gt;
&lt;li&gt;You have a router mounting a mips processor and you want to debug your own DHCP written for it in Go? you know the debugger, true story.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See it? Straightforward!&lt;/p&gt;

&lt;h1 id=&#34;proceed-with-the-autopsy&#34;&gt;Proceed with the Autopsy&lt;/h1&gt;

&lt;p&gt;When you do an autopsy over a crashed Go program there are two things you need:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A core dump&lt;/li&gt;
&lt;li&gt;A non-stripped, with DWARF and debug symbols binary of your program&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To get a core dump, the first thing you do is to check the maximum file size for a core dump file&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ulimit -c
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If it is zero, you are not core dumping so you need to raise the limit.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ulimit -c unlimited
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Also, when debugging, you have to remember that the Go runtime can be said
to trigger certain behavior that can make debugging easier.
In this case, we are interested to say the Go runtime to trigger a core dump
by actually doing a segfault instead of just exiting in case of panic.&lt;/p&gt;

&lt;p&gt;To do so, when we run the Go program we need to run it with &lt;code&gt;GOTRACEBACK=crash&lt;/code&gt; like this.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;GOTRACEBACK=crash ./myprogram
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are also other environment variables and behaviors for &lt;code&gt;GOTRACEBACK&lt;/code&gt;, if
you want to discover more take a look &lt;a href=&#34;https://golang.org/pkg/runtime/#hdr-Environment_Variables&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now that your system can core dump you need to restart the program and wait for it
to crash.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;OR&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;You can obtain the core file of a running program using &lt;code&gt;gcore&lt;/code&gt;,
see &lt;code&gt;man gcore&lt;/code&gt; for more info.&lt;/p&gt;

&lt;p&gt;Core files in linux are written with a template
defined in &lt;code&gt;/proc/sys/kernel/core_pattern&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat /proc/sys/kernel/core_pattern
|/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %e
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In my case it is using the &lt;code&gt;systemd-coredump&lt;/code&gt; program to write files, and I get
lz4 compressed files like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/var/lib/systemd/coredump/core.godebugging.1000.a0b55b870a3f443696cf7cb874d7f27b.32124.1515962062000000.lz4
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;production-just-got-hot&#34;&gt;Production just got hot&lt;/h1&gt;

&lt;p&gt;Generally speaking when a Go binary is built for production most of us
will use something similar to:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;go build -ldflags &amp;quot;-s -w&amp;quot; .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which has the effect of removing the
symbol table and DWARF symbol table from the program.&lt;/p&gt;

&lt;p&gt;Semi OT: if you are a digger, read &lt;a href=&#34;https://dave.cheney.net/2013/10/15/how-does-the-go-build-command-work&#34;&gt;this blog&lt;/a&gt;
from Dave Cheney on what the Go build command does, you&amp;rsquo;ll see a few interesting things.&lt;/p&gt;

&lt;p&gt;The good news is that we can analyze a core file with a binary built from
the same source code, this time without those ldflags.&lt;/p&gt;

&lt;h2 id=&#34;i-don-t-own-the-code&#34;&gt;I don&amp;rsquo;t own the code&lt;/h2&gt;

&lt;p&gt;But in case you don&amp;rsquo;t have or own the source be aware that without a symbol table
we can&amp;rsquo;t debug a program with Delve so with gdb and some Assembly skills you can
use gdb to use the binary you have with the core file you got from the production system,
then you can start analyzing what happened:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-gdb&#34;&gt;gdb ./godebugging core.1234

[New LWP 32124]
[New LWP 32125]
[New LWP 32126]
[New LWP 32127]
[New LWP 32128]
Core was generated by `./godebugging&#39;.
Program terminated with signal SIGABRT, Aborted.
#0  0x0000000000455594 in ?? ()
[Current thread is 1 (LWP 32124)]

(gdb) bt
#0  0x0000000000455594 in ?? ()
#1  0x000000000043d10b in ?? ()
#2  0x0000000000000006 in ?? ()
#3  0x0000000000000000 in ?? ()

(gdb) x/30i $pc
0x455600:    sub    $0x18,%rsp
0x455604:    mov    %rbp,0x10(%rsp)
0x455609:    lea    0x10(%rsp),%rbp
0x45560e:    mov    0x10c96b(%rip),%rax        # 0x561f80
0x455615:    cmp    $0x0,%rax
0x455619:    je     0x45563f
0x45561b:    xor    %edi,%edi
0x45561d:    lea    (%rsp),%rsi
0x455621:    callq  *%rax
0x455623:    mov    (%rsp),%rax
0x455627:    mov    0x8(%rsp),%rdx
0x45562c:    mov    %rax,0x20(%rsp)
0x455631:    mov    %edx,0x28(%rsp)
0x455635:    mov    0x10(%rsp),%rbp
0x45563a:    add    $0x18,%rsp
0x45563e:    retq   
0x45563f:    lea    (%rsp),%rdi
0x455643:    xor    %esi,%esi
0x455645:    mov    0xdaa7c(%rip),%rax        # 0x5300c8
0x45564c:    callq  *%rax
0x45564e:    mov    (%rsp),%rax
0x455652:    mov    0x8(%rsp),%edx
0x455656:    imul   $0x3e8,%rdx,%rdx
0x45565d:    mov    %rax,0x20(%rsp)
0x455662:    mov    %edx,0x28(%rsp)
0x455666:    mov    0x10(%rsp),%rbp
0x45566b:    add    $0x18,%rsp
0x45566f:    retq   
0x455670:    sub    $0x18,%rsp
0x455674:    mov    %rbp,0x10(%rsp)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first thing I did was to use the &lt;code&gt;bt&lt;/code&gt; command to analyze how the backtrace looks
like, and then I showed the next 30 lines of Assembly with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-gdb&#34;&gt;x/30i $pc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All this is not very useful unfortunately but in case of problems it can be of help.&lt;/p&gt;

&lt;h2 id=&#34;i-own-the-code&#34;&gt;I own the code&lt;/h2&gt;

&lt;p&gt;On the other hand if you own the code your life is just easier.&lt;/p&gt;

&lt;p&gt;In this situation, you&amp;rsquo;ll likely to have an optimized binary in production
that can&amp;rsquo;t be used for reading the core dump. If this is the case the thing you can do
is to compile the same source code without the optimizations and then use the debugger
to read the dump file along with it.&lt;/p&gt;

&lt;p&gt;With Delve&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;dlv core ./godebugging core.1234
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;(dlv) bt
0  0x0000000000455594 in runtime.raise
   at /usr/lib/go/src/runtime/sys_linux_amd64.s:113
1  0x0000000000451f70 in runtime.systemstack_switch
   at /usr/lib/go/src/runtime/asm_amd64.s:298
2  0x0000000000427ac8 in runtime.dopanic
   at /usr/lib/go/src/runtime/panic.go:586
3  0x000000000042765e in runtime.gopanic
   at /usr/lib/go/src/runtime/panic.go:540
4  0x00000000004952d9 in main.main
   at ./main.go:34
5  0x00000000004292a6 in runtime.main
   at /usr/lib/go/src/runtime/proc.go:195
6  0x00000000004545c1 in runtime.goexit
   at /usr/lib/go/src/runtime/asm_amd64.s:2337
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Things are so clear, &lt;code&gt;bt&lt;/code&gt; is showing that main.go broke at line 34
leading to a panic, let&amp;rsquo;s see what happened,
the problem seemed to be at frame 4 (the 4 in the backtrace), so I can print the source of that frame:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(dlv) frame 4 list
Goroutine 1 frame 4 at /home/fntlnz/go/src/github.com/fntlnz/godebugging/main.go:34 (PC: 0x4952d9)
    29:         for {
    30:                 select {
    31:                 case message := &amp;lt;-ch:
    32:                         fmt.Println(message)
    33:                 case &amp;lt;-time.After(time.Second * 3):
=&amp;gt;  34:                         panic(&amp;quot;I&#39;m panicking because of timeout!&amp;quot;)
    35:                 }
    36:         }
    37: }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wow! That&amp;rsquo;s a panic, I wrote it so that the program would crash if a timeout occurs, well done program.&lt;/p&gt;

&lt;p&gt;Now I&amp;rsquo;m interested to see the state of the &lt;code&gt;goroutines&lt;/code&gt; at the crash&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dlv&#34;&gt;(dlv) goroutines
[6 goroutines]
* Goroutine 1 - User: ./main.go:34 main.main (0x4952d9) (thread 32124)
  Goroutine 2 - User: /usr/lib/go/src/runtime/proc.go:288 runtime.gopark (0x42974c)
  Goroutine 3 - User: /usr/lib/go/src/runtime/proc.go:288 runtime.gopark (0x42974c)
  Goroutine 4 - User: /usr/lib/go/src/runtime/proc.go:288 runtime.gopark (0x42974c)
  Goroutine 5 - User: ./main.go:21 main.producer (0x4950b0)
  Goroutine 6 - User: /usr/lib/go/src/runtime/proc.go:288 runtime.gopark (0x42974c)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nice, there where six goroutines, a few ones &lt;a href=&#34;https://github.com/golang/go/blob/7c2cf4e779a66b212a3c94f2b20ade1c2c275b84/src/runtime/proc.go#L277&#34;&gt;parked&lt;/a&gt; too&lt;/p&gt;

&lt;p&gt;Between all those goroutines the one t hat seems interesting is the Goroutine 5,
let&amp;rsquo;s see what&amp;rsquo;s inside:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dlv&#34;&gt;(dlv) goroutine 5
Switched from 1 to 5 (thread 32124)
(dlv) bt
0  0x000000000042974c in runtime.gopark
   at /usr/lib/go/src/runtime/proc.go:288
1  0x000000000042983e in runtime.goparkunlock
   at /usr/lib/go/src/runtime/proc.go:293
2  0x0000000000403c0b in runtime.chansend
   at /usr/lib/go/src/runtime/chan.go:222
3  0x0000000000403993 in runtime.chansend1
   at /usr/lib/go/src/runtime/chan.go:113
4  0x00000000004950b0 in main.producer
   at ./main.go:21
5  0x00000000004545c1 in runtime.goexit
   at /usr/lib/go/src/runtime/asm_amd64.s:2337
(dlv) frame 4 ls
Goroutine 5 frame 4 at /home/fntlnz/go/src/github.com/fntlnz/godebugging/main.go:21 (PC: 0x4950b0)
    16: }
    17:
    18: func producer(ch chan&amp;lt;- string) {
    19:         for {
    20:                 time.Sleep(time.Second * time.Duration(rand.Intn(4))) // simulate some work
=&amp;gt;  21:                 ch &amp;lt;- messages[rand.Intn(len(messages)-1)]
    22:         }
    23: }
    24:
    25: func main() {
    26:         ch := make(chan string)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When in the Goroutine 5, I can hook in the &lt;code&gt;frame 4&lt;/code&gt; to see the content of the &lt;code&gt;messages&lt;/code&gt; and &lt;code&gt;ch&lt;/code&gt; variables:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dlv&#34;&gt;(dlv) frame 4 p ch
chan&amp;lt;- string {
        qcount: 0,
        dataqsiz: 0,
        buf: *[0]string [],
        elemsize: 16,
        closed: 0,
        elemtype: *runtime._type {
                size: 16,
                ptrdata: 8,
                hash: 3774831796,
                tflag: 7,
                align: 8,
                fieldalign: 8,
                kind: 24,
                alg: *(*runtime.typeAlg)(0x5420f0),
                gcdata: *1,
                str: 6602,
                ptrToThis: 55936,},
        sendx: 0,
        recvx: 0,
        recvq: waitq&amp;lt;string&amp;gt; {
                first: *sudog&amp;lt;string&amp;gt; nil,
                last: *sudog&amp;lt;string&amp;gt; nil,},
        sendq: waitq&amp;lt;string&amp;gt; {
                first: *(*sudog&amp;lt;string&amp;gt;)(0xc4200a2000),
                last: *(*sudog&amp;lt;string&amp;gt;)(0xc4200a2000),},
        lock: runtime.mutex {key: 0},}
(dlv) frame 4 p messages
[6]string [
        &amp;quot;There&#39;s something that doesn&#39;t make sense. Let&#39;s go and poke it ...+13 more&amp;quot;,
        &amp;quot;We&#39;re all stories, in the end.&amp;quot;,
        &amp;quot;Bow ties are cool.&amp;quot;,
        &amp;quot;One may tolerate a world of demons for the sake of an angel.&amp;quot;,
        &amp;quot;You want weapons? Weâ€™re in a library! Books! The best weapons ...+13 more&amp;quot;,
        &amp;quot;Do what I do. Hold tight and pretend itâ€™s a plan!&amp;quot;,
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With GDB things are similar, but not that nice:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gdb godebugging core.1234
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-gdb&#34;&gt;&amp;gt;&amp;gt;&amp;gt; bt
#0  runtime.raise () at /usr/lib/go/src/runtime/sys_linux_amd64.s:113
#1  0x000000000043d10b in runtime.dieFromSignal (sig=6) at /usr/lib/go/src/runtime/signal_unix.go:400
#2  0x000000000043d299 in runtime.crash () at /usr/lib/go/src/runtime/signal_unix.go:482
#3  0x00000000004281b2 in runtime.dopanic_m (gp=0xc420000180, pc=4355678, sp=842350763480) at /usr/lib/go/src/runtime/panic.go:732
#4  0x0000000000450eac in runtime.dopanic.func1 () at /usr/lib/go/src/runtime/panic.go:587
#5  0x0000000000451ff9 in runtime.systemstack () at /usr/lib/go/src/runtime/asm_amd64.s:344
#6  0x000000000042c3a0 in ?? () at /usr/lib/go/src/runtime/proc.go:1070
#7  0x0000000000544100 in runtime.sched ()
#8  0x00007fff47a3d690 in ?? ()
#9  0x0000000000544160 in ?? ()
#10 0x00007fff47a3d680 in ?? ()
#11 0x000000000042c404 in runtime.mstart () at /usr/lib/go/src/runtime/proc.go:1152
#12 0x0000000000451e21 in runtime.rt0_go () at /usr/lib/go/src/runtime/asm_amd64.s:186
#13 0x0000000000000001 in ?? ()
#14 0x00007fff47a3d6c8 in ?? ()
#15 0x0000000000000001 in ?? ()
#16 0x00007fff47a3d6c8 in ?? ()
#17 0x0000000000000000 in ?? ()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A notable point here is &lt;code&gt;proc.go:1070&lt;/code&gt; so I&amp;rsquo;ll dig into that frame, also it seems that there has been a panic after.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-gdb&#34;&gt;&amp;gt;&amp;gt;&amp;gt; frame 6
#6  0x000000000042c3a0 in ?? () at /usr/lib/go/src/runtime/proc.go:1070
1070	func startTheWorldWithSema() {
&amp;gt;&amp;gt;&amp;gt; l
1065	func mhelpgc() {
1066		_g_ := getg()
1067		_g_.m.helpgc = -1
1068	}
1069	
1070	func startTheWorldWithSema() {
1071		_g_ := getg()
1072	
1073		_g_.m.locks++        // disable preemption because it can be holding p in a local var
1074		gp := netpoll(false) // non-blocking
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What I see is Go code but not mine. Also given that this is a dump of a specific state and not an interactive
debugging session I cannot control the state so I can&amp;rsquo;t even inspect goroutines in such situation.&lt;/p&gt;

&lt;p&gt;The most useful thing I can do now is to get a full backtrace dump printed to a file&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-gdb&#34;&gt;set logging on
bt full
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This creates a &lt;code&gt;gdb.txt&lt;/code&gt; file in the current directory where I see that there&amp;rsquo;s a known variable I can try to print&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-gdb&#34;&gt;&amp;gt;&amp;gt;&amp;gt; p main.messages
$1 = {[0] = &amp;quot;There&#39;s something that doesn&#39;t make sense. Let&#39;s go and poke it with a stick.&amp;quot;, [1] = &amp;quot;We&#39;re all stories, in the end.&amp;quot;, [2] = &amp;quot;Bow ties are cool.&amp;quot;, [3] = &amp;quot;One may tolerate a world of demons for the sake of an angel.&amp;quot;, [4] = &amp;quot;You want weapons? Weâ€™re in a library! Books! The best weapons in the world!&amp;quot;, [5] = &amp;quot;Do what I do. Hold tight and pretend itâ€™s a plan!&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However no luck in getting the right place in my own code where the panic occurred :/&lt;/p&gt;

&lt;h2 id=&#34;the-program-runs-in-a-kubernetes-cluster&#34;&gt;The program runs in a Kubernetes cluster&lt;/h2&gt;

&lt;p&gt;In case your crashed program runs in a kubernetes cluster the good news is that you can just get
your core dumps inside the machines and analyze them like if they weren&amp;rsquo;t containers (unexpected right?).&lt;/p&gt;

&lt;p&gt;The bad news is that in case of a cluster with a number of nodes you will
need some effort to understand on which node the process crasheed and the way
to get the core dump may not be all that straightforward.&lt;/p&gt;

&lt;p&gt;However the other good news is that there&amp;rsquo;s a pull request &lt;a href=&#34;https://github.com/kubernetes/community/pull/1311&#34;&gt;kubernetes/community#1311&lt;/a&gt;
to add a crd just to do that, seems magical!&lt;/p&gt;

&lt;p&gt;In the past on this kind of distributed systems (prior to kubernetes), I was used to
write a program to be used in &lt;code&gt;/proc/sys/kernel/core/core_pattern&lt;/code&gt; that would write the dumps
to a shared filesystem, usually NFS.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using systemd-nspawn for some containerization needs</title>
      <link>https://fntlnz.wtf/post/systemd-nspawn/</link>
      <pubDate>Mon, 14 Nov 2016 10:00:22 &#43;0200</pubDate>
      <author>Lorenzo Fontana</author>
      <guid>https://fntlnz.wtf/post/systemd-nspawn/</guid>
      <description>

&lt;h1 id=&#34;first-things-first&#34;&gt;First things first&lt;/h1&gt;

&lt;p&gt;About one year ago, after years with Fedora 18, I refreshed my laptop and installed a brand new Fedora 22.
My first thought went to all the mess there was before the refresh because I tried tons of applications and changed my mind thousands of times
in those three years.&lt;/p&gt;

&lt;p&gt;This time, I wanted to take my time to &lt;strong&gt;improve the process&lt;/strong&gt; and after a few minutes thinking I had a light-bulb moment and I just started creating &lt;strong&gt;a Dockerfile for every application&lt;/strong&gt; I needed !&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Well, after some time I had 27 images including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;google-chrome&lt;/li&gt;
&lt;li&gt;spotify&lt;/li&gt;
&lt;li&gt;dropbox&lt;/li&gt;
&lt;li&gt;NetworkManager&lt;/li&gt;
&lt;li&gt;pulseaudio&lt;/li&gt;
&lt;li&gt;gnome-terminal-server&lt;/li&gt;
&lt;li&gt;nautilus&lt;/li&gt;
&lt;li&gt;feh&lt;/li&gt;
&lt;li&gt;i3wm&lt;/li&gt;
&lt;li&gt;lightdm&lt;/li&gt;
&lt;li&gt;crond&lt;/li&gt;
&lt;li&gt;VirtualBox&lt;/li&gt;
&lt;li&gt;compton&lt;/li&gt;
&lt;li&gt;parcellite&lt;/li&gt;
&lt;li&gt;guake&lt;/li&gt;
&lt;li&gt;and&amp;hellip; many more!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As you can imagine, I started each one with the right options (&lt;strong&gt;I hope!&lt;/strong&gt;) allowing it to use the X server and other resources.&lt;/p&gt;

&lt;p&gt;In the next days I did some fine tuning and ended up having most of the containers I listed starting as startup system services.&lt;/p&gt;

&lt;h2 id=&#34;what-happened&#34;&gt;What happened ?&lt;/h2&gt;

&lt;p&gt;My computer &lt;strong&gt;took minutes&lt;/strong&gt; to &lt;strong&gt;undefined time&lt;/strong&gt; to boot depending on the state of the Docker daemon, and that wasn&amp;rsquo;t acceptable for me so, sad but full of hope I started thinking at a possible solution
by identifying why Docker wasn&amp;rsquo;t performing well as I expected in such situation.&lt;/p&gt;

&lt;p&gt;The main problem, wasn&amp;rsquo;t that the Docker daemon itself is slow (in fact it isn&amp;rsquo;t) but a mix of factors due to the intrinsic docker&amp;rsquo;s caracteristic that &lt;strong&gt;it wants to manage&lt;/strong&gt; everything for you, like setting up namespaces for existing containers, setting up volumes, managing and connecting to plugins, mounting the layered filesystems, setting up missing network devices and so on..&lt;/p&gt;

&lt;p&gt;All this obviously slows down startup times in certain situations and given the fact that I use docker &lt;em&gt;a lot&lt;/em&gt; for software development and for docker development itself there are a lot of ways that the state of my machine Docker daemon is pretty messy and things are likely to be broken and slow.&lt;/p&gt;

&lt;h1 id=&#34;example-container-spotify&#34;&gt;Example container: Spotify&lt;/h1&gt;

&lt;p&gt;Let&amp;rsquo;s say that I need to listen to some music and I&amp;rsquo;m on Fedora (looks like me now :D)&lt;/p&gt;

&lt;p&gt;I Google for the Spotify Linux client aaaaand that&amp;rsquo;s IT! Spotify does have a Linux client, great!&lt;/p&gt;

&lt;p&gt;Oh, damn, &lt;strong&gt;they only have a Debian package&lt;/strong&gt; :(&lt;/p&gt;

&lt;p&gt;&amp;hellip;Looking for possible solutions&amp;hellip;&lt;/p&gt;

&lt;p&gt;So the first thing I did was in fact to create &lt;strong&gt;a Dockerfile for spotify.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Wait Lorenzo, but &lt;strong&gt;you&amp;rsquo;ve just said you are not using Docker&lt;/strong&gt; for your listening needs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt;: In fact &lt;strong&gt;I don&amp;rsquo;t&lt;/strong&gt;, I&amp;rsquo;m just using Docker to create a Docker image, which I will export to a tar and use as a base filesystem for my container&lt;/p&gt;

&lt;h2 id=&#34;here-s-the-dockerfile&#34;&gt;Here&amp;rsquo;s the Dockerfile:&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-docker&#34;&gt;FROM debian:jessie

RUN apt-get update -y
RUN gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys BBEBDCB318AD50EC6865090613B00F1FD2C19886
RUN gpg --export --armor BBEBDCB318AD50EC6865090613B00F1FD2C19886 | apt-key add -
RUN echo deb http://repository.spotify.com stable non-free | tee /etc/apt/sources.list.d/spotify.list
RUN apt-get update -y
RUN apt-get install spotify-client -y
RUN apt-get install pulseaudio -y
RUN apt-get install -f -y
RUN echo enable-shm=no &amp;gt;&amp;gt; /etc/pulse/client.conf

ENV PULSE_SERVER /run/pulse/native
ENV HOME /home/spotify

RUN useradd --create-home --home-dir $HOME spotify \
  &amp;amp;&amp;amp; gpasswd -a spotify audio \
  &amp;amp;&amp;amp; chown -R spotify:spotify $HOME

  WORKDIR $HOME
  USER spotify
  ENTRYPOINT  [ &amp;quot;spotify&amp;quot; ]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;run-the-thing&#34;&gt;Run the thing&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -d \
  -v /etc/localtime:/etc/localtime:ro \
  -v /tmp/.X11-unix:/tmp/.X11-unix \
  -e DISPLAY=unix$DISPLAY \
  -v /run/user/1000/pulse:/run/pulse:ro \
  -v /var/lib/dbus:/var/lib/dbus \
  -v $HOME/.spotify/config:/home/spotify/.config/spotify \
  -v $HOME/.spotify/cache:/home/spotify/spotify \
  --name spotify \
  fntlnz/spotify
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that I have my image and I can use it with Docker seeing that it works I can try it with &lt;code&gt;systemd-nspawn&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The first thing to do is to export the docker image to a folder we&amp;rsquo;ll call &lt;code&gt;rootfs&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p /var/lib/machines
cd /var/lib/machines

mkdir spotify
docker export $(docker create fntlnz/spotify) | tar -C spotify -xvf -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we have to give the right permissions to &lt;code&gt;/home/spotify&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;systemd-nspawn -D spotify/ bash -c &amp;quot;chown -R spotify:spotify /home/spotify&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now each time we want to start that container we can do it with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;systemd-nspawn \
  --setenv=DISPLAY=unix$DISPLAY \
  --bind=/tmp/.X11-unix:/tmp/.X11-unix \
  --bind /run/user/1000/pulse:/run/pulse \
  --bind /var/lib/dbus:/var/lib/dbus \
  -u spotify -D spotify/ \
  spotify
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;additional-notes&#34;&gt;Additional notes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;We haven&amp;rsquo;t used any layered filesystem and the container is actually writing into the &lt;code&gt;spotify&lt;/code&gt; directory.&lt;/li&gt;
&lt;li&gt;The network stack is not isolated&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;1000&lt;/strong&gt; user id needs to be changed with the id of the user connected to the X session (your user id on that machine)&lt;/li&gt;
&lt;li&gt;I&amp;rsquo;m not mounting &lt;code&gt;$HOME/.spotify&lt;/code&gt; things inside my &lt;code&gt;systemd-nspawn&lt;/code&gt; container since I decided to keep the state in the &lt;code&gt;spotify&lt;/code&gt; directory&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;machinectl&#34;&gt;machinectl&lt;/h1&gt;

&lt;p&gt;There&amp;rsquo;s another tool, invokable via &lt;code&gt;machinectl&lt;/code&gt; which allows you to manage your &amp;ldquo;machines&amp;rdquo; aka containers and vms managed
by the &lt;a href=&#34;https://wiki.freedesktop.org/www/Software/systemd/machined/&#34;&gt;&lt;strong&gt;systemd machine manager&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;container-services&#34;&gt;Container services&lt;/h2&gt;

&lt;p&gt;Using machinectl you can even create startup services, for example I use this for the NetworkManagr (image not included)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;machinectl enable network-manager
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Output:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Created symlink from /etc/systemd/system/machines.target.wants/systemd-nspawn@network-manager.service to /usr/lib/systemd/system/systemd-nspawn@.service.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;management&#34;&gt;Management&lt;/h2&gt;

&lt;p&gt;machinectl allows you to list, terminate and show the status of machines.&lt;/p&gt;

&lt;h3 id=&#34;list-all-the-machines&#34;&gt;List all the machines&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;machinectl list
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Output:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MACHINE CLASS     SERVICE
spotify container nspawn 

1 machines listed.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;terminate-the-machine&#34;&gt;Terminate the machine&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;machinectl terminate spotify
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;get-the-status&#34;&gt;Get the status&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;machinectl status spotify
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Output:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;spotify
           Since: Mon 2016-11-14 02:13:54 CET; 7s ago
          Leader: 11308 (spotify)
         Service: nspawn; class container
            Root: /var/lib/machines/spotify
              OS: Debian GNU/Linux 8 (jessie)
            Unit: machine-spotify.scope
                  â”œâ”€11308 /usr/share/spotify/spotif
                  â”œâ”€11321 /usr/share/spotify/spotify --type=zygote --no-sandbox --lang=en-US --log-file=/usr/share/spotify/debug.log --log-severity=disable --product-version=Spotify/1.0.42.151
                  â”œâ”€11344 /proc/self/exe --type=gpu-process --channel=1.0.1413324922 --mojo-application-channel-token=7BA7725BB9581D934FDAECBCAC0E2C8B --no-sandbox --window-depth=24 --x11-visual-id=32 --lang=en-
                  â””â”€11372 /usr/share/spotify/spotify --type=renderer --disable-pinch --no-sandbox --primordial-pipe-token=431AFC3F7268B33A8765213F7926A54A --lang=en-US --lang=en-US --log-file=/usr/share/spotify/

Nov 14 02:13:54 fntlnz systemd[1]: Started Container spotify.
Nov 14 02:13:54 fntlnz systemd[1]: Starting Container spotify.
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;pull-images&#34;&gt;Pull images&lt;/h4&gt;

&lt;p&gt;machinectl can pull images using &lt;code&gt;pull-raw&lt;/code&gt;, &lt;code&gt;pull-tar&lt;/code&gt; and &lt;code&gt;pull-dkr&lt;/code&gt; from remote urls.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;machinectl pull-raw --verify=no http://ftp.halifax.rwth-aachen.de/fedora/linux/releases/21/Cloud/Images/x86_64/Fedora-Cloud-Base-20141203-21.x86_64.raw.xz
systemd-nspawn -M Fedora-Cloud-Base-20141203-21
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;for more, see &lt;a href=&#34;https://www.freedesktop.org/software/systemd/man/machinectl.html&#34;&gt;machinectl&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;what-s-next&#34;&gt;What&amp;rsquo;s next&lt;/h1&gt;

&lt;p&gt;In this post I showed you something like the top 1% of the things that can be done with &lt;code&gt;systemd-nspawn&lt;/code&gt;, there&amp;rsquo;s &lt;strong&gt;moar!!&lt;/strong&gt;, like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Usage of btrfs as container root and ephemeral containers&lt;/li&gt;
&lt;li&gt;Network isolation and advanced network interfaces (macvlan, ipvlan)&lt;/li&gt;
&lt;li&gt;Integration with SELinux&lt;/li&gt;
&lt;li&gt;bootable images&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;see &lt;code&gt;man machinectl&lt;/code&gt; and &lt;code&gt;man systemd-nspawn&lt;/code&gt; for more&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;what-i-achieved&#34;&gt;What I achieved ?&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Portability and zero setup time&lt;/strong&gt;: most linux distributions today are using systemd that means that my containers will work without having to install anything&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Faster startup times&lt;/strong&gt;: as I said, starting just a process is faster than setting up a Docker container, not because Docker is slow (it&amp;rsquo;s not) but because it does actually more than just starting the process&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reuse&lt;/strong&gt;: I can reuse any docker image I want just by exporting it to a tar file, I can even reuse virtual machine images.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now let&amp;rsquo;s listen some music!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://fntlnz.wtf/systemd-nspawn/video.gif&#34; alt=&#34;systemd nspawn spotify&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why do we have containers</title>
      <link>https://fntlnz.wtf/post/why-containers/</link>
      <pubDate>Thu, 11 Aug 2016 15:35:53 CEST</pubDate>
      <author>Lorenzo Fontana</author>
      <guid>https://fntlnz.wtf/post/why-containers/</guid>
      <description>

&lt;h1 id=&#34;disclaimer&#34;&gt;Disclaimer&lt;/h1&gt;

&lt;p&gt;This post reflects my own view of the whole world of virtualization, I summed up here my thoughts but please
if you find something that you consider wrong or inexact leave a comment so I can learn by you and improve myself.&lt;/p&gt;

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;While talking about Linux Containers, Docker Containers or more in general about the &lt;strong&gt;concept of containers itself&lt;/strong&gt; I often (as it should be) encounter doubts and questions like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What is the actual difference between containers and other types of virtualization?&lt;/li&gt;
&lt;li&gt;How are containers going to make my life better?&lt;/li&gt;
&lt;li&gt;What should I say to my security team while I&amp;rsquo;m using those containers?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All this can be summarized in a more simple question:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Why do we have containers ? What we had before was not enough?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this post I want to try to explain why, in my opinion, &lt;strong&gt;OS Level Virtualization&lt;/strong&gt; (aka. containers) is a thing that matters now
primarily by analyzing the details of each virtualization method.&lt;/p&gt;

&lt;h1 id=&#34;different-types-of-virtualization&#34;&gt;Different types of virtualization&lt;/h1&gt;

&lt;p&gt;At the moment there are three main distinct types of Virtualization, namely:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Full Virtualization&lt;/li&gt;
&lt;li&gt;Paravirtualization&lt;/li&gt;
&lt;li&gt;OS Level Virtualization (Containers)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The most interesting one for the discussion is the last one but is important to understand that at some extent all types have two common denominators:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://fntlnz.wtf/why-containers/Virtualization.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A Host: the primary operating system where the Docker daemon is running, where KVM is running, where VMware is running&lt;/li&gt;
&lt;li&gt;A Guest: the actual container or virtual machine&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The main difference between all those types of virtualization is the level at which them interact with the underlying resources and devices
thus is responsible for their difference in terms of security, resources usage and portability.&lt;/p&gt;

&lt;p&gt;This discussion can be easily understood in the context of load balancers where there two types of them: Hardware and software load balancers,
both do load balancing and while the hardware version typically provides improved performances this comes at the cost of another specific piece of hardware with the limitations of the case.
as opposite, software load balancers can be installed on any hardware, are more portable, customizable and probably fits better specific needs.&lt;/p&gt;

&lt;p&gt;Given all this, there are a few areas of differentiation between all the different types of virtualization:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: Having a full operating system with its own kernel may seem a source of additional security because of the fact
that is nearly impossible to escape from virtual machines. But if you consider the fact that you are putting a &lt;strong&gt;brand new kernel, with its vulnerabilities
and bugs&lt;/strong&gt; on top of your existing kernel you may change your point of view. On the other hand, OS-Level virtualization share the same kernel with the host operating system
but comes with a larger surface for possible attackers due to the syscalls, shared networking, device and disk access. Anyway don&amp;rsquo;t worry most of these security issues are
solved in most used OS-Level virtualization platforms such as Docker, in fact Docker Containers for example are fully integrated with cgroups, seccomp, SELinux, and all the possible resources
are isolated properly using Kernel namespaces, also you can manually add or drop capabilities.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Portability&lt;/strong&gt;: Full virtualization and paravirtualization both leverages on specific hardware, because of its nature requires more resources than
OS-Level virtuanchorlinenosalization which can be used anywhere a modern Linux kernel is present.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt;: The fact that OS-Level virtualization works in a shared kernel architecture offers super fast startup times, this is not true for other types of virtualization
where starting a new system is a lot more than just spawning a new process as it is with containers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Application Portability&lt;/strong&gt;: as because containers can be moved easily due to their decreased size due to the fact that they don&amp;rsquo;t own an entire operating system and a kernel
we can obtain &lt;strong&gt;reduced downtimes&lt;/strong&gt; and avoid headaches. For other types of virtualization this is not true, we all know that moving a virtual machine to another
host requires lots of bandwidth and storage.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: given that OS-Level virtualization implies the fact that the kernel is shared usually there&amp;rsquo;s an implemented Copy on Write filesystem and Union Filesystem
to put all the pieces toghether and obtain containers that only owns their code, components and libraries.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Live Migration&lt;/strong&gt;: Some OS-Level virtualization platform such as OpenVZ does support live migrations, most Fill virtualization platforms supports it. Docker containers on the
other hand does not have an official implementation for this&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;motivations-for-using-containers&#34;&gt;Motivations for using Containers&lt;/h2&gt;

&lt;p&gt;I can&amp;rsquo;t know what is your intended use case.
But if at some extent your motivation consist in &lt;strong&gt;decreasing costs&lt;/strong&gt; related to full virtualization overheads
while &lt;strong&gt;allowing developers to ship, develop and test code faster&lt;/strong&gt; we already found two.&lt;/p&gt;

&lt;h2 id=&#34;in-the-end-why-containers-are-actually-there&#34;&gt;In the end, why containers are actually there?&lt;/h2&gt;

&lt;p&gt;If you&amp;rsquo;v read up to here you probably think I missed the point, instead what I wanted to write is exactly that
OS-Level virtualization (aka containers) is just another way of doing virtualization which solves specific needs that are not solved by others:&lt;/p&gt;

&lt;p&gt;We need containers for being able to work closer to the kernel while executing in a fully isolated environment.
This specific thing allows us to achieve &lt;strong&gt;higher densities&lt;/strong&gt; and run more workload within the same hardware while
obtaining &lt;strong&gt;faster delivery and scaling&lt;/strong&gt; thanks to their nature of high portability.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
